---
layout: single
title: "ML - Lecture 2: Point estimation"
categories: Machine_learning
tags: ML
toc: true
author_profile: false
---

# Estimators
* 추정량
  * Point estimation: Single best prediction을 얻기 위한 시도
  * Bayesian learning: 확률 분포를 포함하고 있음(불확실성을 내포)<br>
    ☞Point estimation이 확률 값 자체라면 bayesian은 분포표까지 생각하는 것

____

# Maximum likelihood estimation(MLE) for a Binomibal distribution

Machine learning에서 확률을 표현하는 기호로는 $\theta$로 사용한다.<br>
만약 Head, Tail만 경우의 수가 나타나는 것으로 했을 때,<br>
<br>
**<center> P(D|$\theta$) = $\theta^{a_H} (1-\theta)^{a_T}$ </center>**<br>
MLE : Choose $\theta$ that maximizes the likelihood of observed data (빈도가 제일 큰부분의 위치 $\theta$를 찾아냄)<br>
**<center> $\hat{\theta}$ = arg max P(D|$\theta$) = arg max \ln P(D|$\theta$)</center>**<br>
☞ arg max는 함수값이 최대가 되는 위치를 알려줌. $f(x)$가 최대일 때 $x$값을 리턴한다는 뜻<br>

우리가 함수의 local maximum을 찾을 때 미분값이 0이 되는 숫자를 찾는것 처럼 $\hat{\theta}$를 구할 수 있다.<br>
$\frac{d}{d\theta}$ln P(D|$\theta$) = 0 $\to$ $\frac{a_H}{\theta} + \frac{-a_T}{1 - \theta}$ = 0<br>
$a_H - a_H\theta - a_T\theta$ = 0, $\hat{\theta}$ = $\frac{a_H}{a_H + a_T}$

**<center> $\hat{\theta}_{MLE}$ = $\frac{a_H}{a_H + a_T}$</center>**

___

# Hoeffding's Inequality (PAC, Probably Approximately Correct)
$N = a_H + a_T =$ 시행횟수인데 이게 커질수록 정확도가 올라가지 않을까?<br>
그래서 Hoeffding's Inequality를 이용을 하는거다.<br>
**<center> $P(\vert \hat{\theta}_{MLE} - \theta^*\vert \geq \epsilon) \leq 2e^{-2N\epsilon^2}$</center>**<br>

완벽한 solution -> $\theta^*$ :<br>
errors -> $\vert \hat{\theta}_{MLE} - \theta^*\vert$<br>
오차범위 -> $\epsilon$<br>
즉, N이 커질수록 점점 정확해지는 것이다.<br>
_____

# MLE for continuous variables(Gaussians)
* Gaussians<br>
  $f_{\mu, \sigma^2}(x_i) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(x_i-\mu)^2}{2\sigma^2}\right)$<br>
그래서 MLE를 Gaussian적용하면<br>
**<center> $P(x|\theta) = \prod_{i=1}^{n}f_{\mu, \sigma^2}(x_i) = \prod_{i=1}^{n}\frac{1}{\sigma\sqrt{2\pi}} \exp \left( -\frac{(x_i-\mu)^2}{2\sigma^2} \right)$</center>**<br>
가 되고 log를 취한 꼴은 
**<center>$L(\theta|x) = \sum_{i=1}^{n}\log\frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(x_i-\mu)^2}{2\sigma^2}\right)$</center>**<br>
가 된다.그래서 $\mu$에 대해 편미분하고 $\sigma$에 대해 편미분을 통해 0이되는 값을 찾으면<br>
**<center>${\hat{\mu}}_{MLE} = \frac{1}{n}\sum_{i=1}^{n}x_i , {\hat{\sigma}^2}_{MLE} = \frac{1}{n}\sum_{i=1}^{n}\left(x_i-\mu\right)^2$</center>**<br>
를 구할 수 있다.<br>
추가적으로 unbiased된 것은 다음과 같이 나타낸다.
**<center>${\hat{\sigma}^2}_{unbiased} = \frac{1}{n}\sum_{i=1}^{n-1}\left(x_i-\mu\right)^2$</center>**<br>