---
layout: single
title: "ML - Lecture 2: Point estimation"
categories: Machine_learning
tags: ML
toc: true
author_profile: false
---

# Estimators
* 추정량
  * Point estimation: Single best prediction을 얻기 위한 시도
  * Bayesian learning: 확률 분포를 포함하고 있음(불확실성을 내포)<br>
    ☞Point estimation이 확률 값 자체라면 bayesian은 분포표까지 생각하는 것

____

# Maximum likelihood estimation(MLE) for a Binomibal distribution

Machine learning에서 확률을 표현하는 기호로는 $\theta$로 사용한다.<br>
만약 Head, Tail만 경우의 수가 나타나는 것으로 했을 때,<br>
<br>
**<center> P(D|$\theta$) = $\theta^\a_H (1-\theta)^\a_T$ </center>**
MLE : Choose $\theta$ that maximizes the likelihood of observed data (빈도가 제일 큰부분의 위치 $\theta$를 찾아냄)<br>
**<center> $\hat{\theta}$ = arg max P(D|$\theta$) = arg max \ln P(D|$\theta$)</center>**
☞ arg max는 함수값이 최대가 되는 위치를 알려줌. $f(x)$가 최대일 때 $x$값을 리턴한다는 뜻<br>

우리가 함수의 local maximum을 찾을 때 미분값이 0이 되는 숫자를 찾는것 처럼 $\hat{\theta}$를 구할 수 있다.<br>
$\frac{d}{d\theta}$ln P(D|$\theta$) = 0 $\to$ $\frac{\a_H}{\theta} + \frac{-\a_T}{1 - \theta}$ = 0<br>
$\a_H - \a_H/theta - \a_T\theta$ = 0, $\hat{\theta}$ = $\frac{\a_H}{\a_H + \a_T}$

**<center> $\hat{\theta}$ = $\frac{H}{H + T}$</center>**

___

# Hoeffding's Inequality
