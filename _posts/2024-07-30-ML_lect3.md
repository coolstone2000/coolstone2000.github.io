---
layout: single
title: "ML - Lecture 3: Linear regression"
categories: Machine_learning
tags: ML
toc: true
author_profile: false
---

# Linear regression

regression(회귀): 변수들 사이에서 나타나는 경향성을 설명하는 것<br>
일단 진행하다 보면 무조건 평균으로 회귀를 한다는 것인데 왜그런진 모른다. 그냥 그런거다.<br>

## The model
일단 집값 예측 모델로 예시를 들어 설명할 것이다.<br>
우선 어떤 model이든 data를 이용하여 만들게 되는데 input x에는 거의 y에 뭔가 영향을 미치는 feature를 넣고 output y는 우리가 궁금해 하는 것의 수치이다.<br>
<center><img src="/images/ML/ML_lr_hs_r.png"></center>
이렇게 data들을 점으로 표시해 내가 해당되는 근처의 range만 뽑아서 그 근처의 값으로만 y 값을 추정하게 되면 아주 많은 data를 모은것에 비해 실제 사용한 data는 굉장히 적다.<br>
<center><img src="/images/ML/ML_lr_r_modle_ex.png"></center>
그래서 우리는 위와 같은 regression model을 사용하는 것이다.<br>

- - -
* **Machine learning flowchart**
<center><img src="/images/ML/ML_flowchart.png"></center>
위의 flowchart는 근본적인 machine learning에 대한 것이다. 설명을 하자면 우선 **Training data**에서 y값에 영향을 줄만한 **feature**들을 뽑아 x로 만들어 준다. 그 뽑은 data들로 machine learning model을 만들게 된다. 하지만 이 model이 무조건 좋은 모델인지는 모른다. 그래서 이것을 신뢰도를 높이기 위해 **Quality metric**이란 것을 하여 model을 약간씩 수정하게 된다. Quality metric은 **거리를 잰다**는 것인데 즉 ML model이 충분히 좋아질 때 까지 **$\hat{f}$**를 찾는 것이다. 이것은 y와 $\hat{y}$을 비교하여 찾고 계속 **update해서 오차를 점점 줄여나가는 것**이다. 이것이 모든 machine learning의 기초 개념이다.
- - -
# simple linear regressinon model
<center><img src="/images/ML/ML_lr_2d_ex.png"></center>
우리가 평소에 그래프를 해석하거나 찾을 때는 x나 y에 대해서 관심있고 궁금해하였다. 하지만 ML에서는 다른곳에 관심을 가져야 한다. 바로 여기서 w로 된 값들이다. 이 w로 된 값들을 **parameter of model** 이나 **regression coefficient**라고 한다. 어짜피 x와 y값은 연속적으로 다 구할 수 있고 그래프의 모양은 오히려 이 계수들에 의해 결정되게 된다. 그래서 우리는 **w를 잘 찾는 것**에 대해 관심을 가져야 한다. 이렇게 model을 하나 만들었다고 하면 그 x지점에서 model과 실제 data값의 차이가 생기게 될 것이다. 이는 **noise($\epsilon_i$)**라고 한다. 그래서 위의 flowchart에서 $\hat{f}$를 찾는 것이라고 하였는데 이제 우리는 이$\hat{f}$을 **$\hat{w}$**로 나타낼 것이다.

## Cost
일단 noise($\epsilon_i$)가 작은 line을 찾았다면 그 line은 경향을 잘 보여주는 라인이라고 할 수 있지 않을까? 그래서 이 noise를 cost라고 한다. 이 cost를 최대한 줄일 수 있는 w를 찾아야 하는 것이다.