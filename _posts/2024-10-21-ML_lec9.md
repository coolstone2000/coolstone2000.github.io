---
layout: single
title: "ML - Lecture 9: Linear classifier - overfitting and regularization"
categories: Machine_learning
tags: ML
toc: true
author_profile: false
---

이렇게 모델을 만들고나면 우리가 하는게 있다. overfitting 되었는가와 그것을 regularization으로 해결하는 것이다. 그걸 또 logistic regression에서 해보려고 한다.
<br>
<br>

# ◼︎ Overfitting in classification

앞에서 overfit이 언제 되었는지 기억하는가? w의 영향이 너무 세질때 생기게 된다고 했다. 그래서 여기도 w의 값이 너무 커질수록 overfitting이 일어나게 된다.
<center><img src="/images/ML/lr_overfit.png"></center>

<details>
<summary><del>???: 에헤이 조졌네 이거</del></summary>
<iframe width="400" height="500" src="https://www.youtube.com/embed/_A_TBtBZuTQ" title="주기적으로 봐야하는 영상    #조졌네" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></details>
<br>
<br>

# ◼︎ Overconfident prediction

Logistic regression은 overfitting과 비슷한게 있다. 바로 **overconfident**이다. 모델을 과하게 신뢰하는 것이다.
<center><img src="/images/ML/lr_overconfi.png"></center>
이렇게 좁은 영역에서 overfitting이 일어나고 overconfident한 prediction을 내리게 된다.<br>
<br>

# ◼︎ Penalizing large coefficients to mitigate overfitting

우리가 cost function을 정의했듯이 비슷한걸 할 것이다. 
> **<center><span style="font-size:115%">Total quality = measure of fit - measure of manitude of coefficients</span></center>**

그래서 다음과 같이 쓸 것이다.

<center><span style="font-size:150%">Total quality = $ll(w) - \lambda \left\| w \right\|^2_2$</span></center>
이것을 gradient를 씌우면 다음과 같이 된다.
<center><img src="/images/ML/lr_logi_regul.png"></center>
이 결과를 이용해 regularization을 적용하면 이렇게 된다.