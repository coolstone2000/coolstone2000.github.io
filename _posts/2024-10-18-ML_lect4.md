---
layout: single
title: "ML - Lecture 4: Linear regression part 2"
categories: Machine_learning
tags: ML
toc: true
author_profile: false
---

# â—¼ï¸ Assessing performance

ìš°ë¦¬ëŠ” modelê³¼ algorithmì„ í†µí•´ fitted functionì„ ì°¾ì•„ë‚¸ë‹¤. ì´ë ‡ê²Œ functionì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì„ **prediction**ì„ í†µí•´ í•˜ëŠ”ë° ë§Œì•½ perfect preditionì´ë¼ë©´ perfectionê³¼ ë¹„êµí–ˆì„ ì‹œì— **lossëŠ” 0**ì´ê² ì§€ë§Œ ì‹¤ì œë¡œ ë‚´ê°€ ì°¾ì€ perfectionì€ ê·¸ë ‡ì§€ ì•Šë‹¤. <br>

## Measuring loss

* Loss function: $L(y, f_{\hat{w}}(x))$
* Actual value: $y$
* Predicted value: $\hat{y} = f_{\hat{w}}(x)$<br>

ê·¸ë˜ì„œ ì´ loss functionì„ ì–´ì¼€ ë‘ëŠëƒì¸ë° ì „ lectureì—ì„œ ì–¸ê¸‰í•œ ê·¸ê²ƒì´ ë‚˜ì˜¨ë‹¤.<br>
* Absolute error: 
$L(y, f_{\hat{w}}(x)) = |y - f_{\hat{w}}(x)|$ 
**(L1-norm)**
* Squared error: $L(y, f_{\hat{w}}(x)) = (y - f_{\hat{w}}(x))^2$ **(L2-norm)**<br>

ê·¸ëŸ°ë° ë­”ê°€ parameterë¥¼ ê³„ì† ë§ì´ ë„£ì–´ì£¼ë©´ ì •ë³´ê°€ ë§ìœ¼ë‹ˆê¹Œ ë” ì •í™•í•´ì§€ê³  ì¢‹ì€ ëª¨ë¸ì´ ë‚˜ì˜¬ê²ƒ ê°™ì§€ ì•Šì„ê¹Œ? ì´ê±´ ì™„ì „íˆ ì˜ëª»ëœ ìƒê°ì´ë‹¤. ëª‡ ê°œ ì¶”ê°€í• ë•ŒëŠ” ê½¤ë‚˜ regresssionì´ ì •í™•í•´ì§€ëŠ” ê²ƒ ì²˜ëŸ¼ ë³´ì¼ ê²ƒì´ë‹¤. ê·¸ëŸ°ë° ë‹¤ìŒ ê·¸ë¦¼ê³¼ ê°™ì€ ìƒí™©ì´ë©´ ì–´ë–¨ê¹Œ?<br>

<center><img src="/images/ML/ML_lr_bad_pre.png" width = "700"></center><br>

ê± ëˆˆìœ¼ë¡œë§Œ ë´ë„ ê²½í–¥ì„±ì„ ì „í˜€ ì•ˆë”°ë¥´ê³  ìˆë‹¤ëŠ” ê²ƒì´ ë³´ì´ì§€ ì•Šì€ê°€? ê·¸ë˜ì„œ ë§ˆêµ¬ì¡ì´ë¡œ ëŠ˜ë¦¬ê¸°ë§Œ í•´ì„œëŠ” ì•ˆë˜ëŠ” ê²ƒì´ë‹¤.<br>
<br>

# â—¼ï¸ Assessing the loss

## Part 1: Training error

<center><img src="/images/ML/ML_lr_defining.png" width = "700"></center><br>

ì´ê²ƒì€ **defining training data**ë¼ê³  í•˜ëŠ”ë° í•™ìŠµì„ ì‹œí‚¬ ë•Œ ëª¨ë“  dataë¥¼ ë‹¤ ë„£ëŠ” ê²ƒì´ ì•„ë‹Œ ì„ íƒì ìœ¼ë¡œ ë„£ëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ ì´ ê·¸ë¦¼ì—ì„œëŠ” íŒŒë€ìƒ‰ dataë¥¼ í•™ìŠµì— í¬í•¨ì‹œí‚¤ì§€ ì•ŠëŠ”ë‹¤.<br>

### â€¢ Compute training error

1. Define a loss function $L(y, f_{\hat{w}}(x))$ (ì–´ë–¤ ì¢…ë¥˜ë¡œ í• ì§€)
2. Training error = avg.loss in training set = $\frac{1}{N_{training}} \sum_{i=1}^{N_{training}}L(y, f_{\hat{w}}(x))$<br>

### â€¢ Training error vs model complexity

<center><img src="/images/ML/ML_lr_te_mc.png" width = "700"></center><br>
ì´ê±¸ ë³´ë©´ ì•Œ ìˆ˜ ìˆë“¯ model complexityê°€ ë†’ì•„ì§ˆ ìˆ˜ë¡ errorëŠ” ê°ì†Œí•œë‹¤. ê·¸ëŸ°ë° ê³¼ì—° ì´ë ‡ê²Œ training dataë¥¼ ë½‘ì•„ì„œ ë§Œë“  ëª¨ë¸ì˜ errorê°€ ì‘ì€ê²Œ good predictionì´ë¼ê³  í•  ìˆ˜ ìˆì„ê¹Œ?<br>

ë‹µì€ <span style="background-color:#DB9239">**ì ˆëŒ€ ì•„ë‹ˆë‹¤**</span>ì´ë‹¤. Training dataê°€ ì „ìˆ˜ì¡°ì‚¬ë¥¼ í•œ ê²ƒ ì•„ë‹ˆë©´ ì¢‹ì§€ ì•Šë‹¤. ê·¼ë° ì¼ë‹¨ì€ ì „ìˆ˜ì¡°ì‚¬ ìì²´ê°€ ë¶ˆê°€ëŠ¥í•˜ë‹¤.<br>

## Part 2: Generalization (true) error

ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì€ ëª¨ë“  ê°€ëŠ¥í•œ lossë¥¼ estimateí•˜ëŠ” ê²ƒì´ë‹¤. ì¦‰, datasetì— ì—†ëŠ” ê²ƒì„ ì˜ ì¶”ì •í•´ì•¼í•˜ëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ generalization errorëŠ” ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ëŠ” ì˜ ë™ì‘í•˜ì§€ë§Œ ìƒˆë¡œìš´ ë°ì´í„°(datasetì— ì—†ë˜ ê²ƒ)ì— ëŒ€í•´ ì œëŒ€ë¡œ ì¼ë°˜í™” í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ì¸ ê²ƒì´ë‹¤. <br>

### â€¢ Generalization (true) error definition

>**<center>Generalization (true) error = $E_{x,y}[L(y,f_{\hat{w}}(x))] = \int L(y,f_{\hat{w}}(x))p(x,y) \, dxdy$</center>**

* $E_{x,y}$: Average over all possible (x, y) pairs
* $p(x, y)$: computationally not possible

### â€¢ Generalization (true) error vs model complexity

<center><img src="/images/ML/ML_lr_gen_mc.png" width = "700"></center><br>
Generalization errorëŠ” ëª¨ë¸ì´ ë³µì¡í•´ì§ˆìˆ˜ë¡ ë‚®ì•„ì§€ë‹¤ê°€ ë˜ ë„ˆë¬´ ë§ì´ ë³µì¡í•´ì ¸ë²„ë¦¬ê²Œ ë˜ë©´ ë‹¤ì‹œ errorê°€ ë†’ì•„ì§„ë‹¤.<br>

### â€¢ Training vs Generalizaion

ê·¸ë˜ì„œ ë‹¤ìŒ ê·¸ë˜í”„ë“¤ì€ training errorì™€ generalization errorë¥¼ ë¹„êµí•œ ê·¸ë¦¼ì´ë‹¤.
<center><img src="/images/ML/ML_lr_te_vs_gen.png" width = "700"></center><br>

## Part 3: Test error

True errorëŠ” ëª¨ë“  datasetì— ëŒ€í•˜ì—¬ ë¶„ì„ì„ í–ˆë‹¤. ê·¸ëŸ°ë° ì´ê±¸ ì‹¤ì œë¡œ í•˜ëŠ”ê±´ ë¶ˆê°€ëŠ¥ì— ê°€ê¹ê³  ê³„ì‚° ì–‘ì´ ë„ˆë¬´ ë§ì§€ ì•Šì„ê¹Œ? ê·¸ë˜ì„œ ê³ ì•ˆëœê²ƒì´ **<span style="background-color:#DB9239">test set</span>**ì´ë‹¤. ìš°ë¦¬ê°€ ê°€ì§€ê³  ìˆëŠ” datasetì„ training setê³¼ test setë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ë‹¤. ì¦‰, **data set = training set + test set**. ì´ë ‡ê²Œ ë”°ë¡œ testsetì„ ë¹¼ëŠ” ê²ƒì—ëŠ” ì´ ê²ƒë“¤ì´ ì „ì²´ë¥¼ ëŒ€ë³€í•  ìˆ˜ ìˆì„ ê²ƒì´ë¼ëŠ” ê°€ì •ì´ ìˆë‹¤. ê·¸ë˜ì„œ true error ì˜ approximationì€ test errorê°€ ëœë‹¤.<br>

### â€¢ Compute test error

* Test error = avg.loss in test set = $\frac{1}{N_{test}} \sum_{i\;in \; testset}L(y, f_{\hat{w}}(x))$

<center><img src="/images/ML/ML_lr_overfit.png" width = "700"></center><br>
ìœ„ì˜ ê·¸ë˜í”„ì—ì„œ ê²€ì€ìƒ‰ ê·¸ë˜í”„ëŠ” true errorì— ê·¼ì ‘í•˜ê³  ìˆëŠ” test errorë¥¼ ë³´ì—¬ì¤€ ê²ƒì´ë‹¤. True errorì— ë¹„í•´ test errorëŠ” noiseê°€ ê»´ìˆë‹¤.ì´ ê·¸ë˜í”„ë“¤ë¡œ ì–¼ë§ˆë‚˜ fití•œì§€ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤.<br>

* Underfitting (ê³¼ì†Œì í•©): training setì´ ì œëŒ€ë¡œ í•™ìŠµë˜ì§€ ì•Šì•„ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” ê²½ìš°
    - Training error(w) > Training error(w')
    - True error(w) > True error(w')
    - ìœ„ ë‘ê°œë¥¼ ë§Œì¡±í•˜ë©´ underfitted
<br>
<br>
* Overfitting (ê³¼ëŒ€ì í•©): í•™ìŠµ ëª¨ë¸ì´ ì§€ë‚˜ì¹˜ê²Œ training dataì—ë§Œ ì´ˆì ì´ ë§ì¶°ì ¸ ì¼ë°˜í™” ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” ê²½ìš°
    - Training error(w) > Training error(w')
    - True error(w) <> True error(w')
    - ìœ„ ë‘ê°œë¥¼ ë§Œì¡±í•˜ë©´ overderfitted
<br>
<center><img src="/images/ML/Meat_even.gif" width = "700"></center><br>
**~~???: evení•˜ê²Œ ìµì§€ ì•Šì•˜ì–´ìš”~~**

## Error vs amount of data

<center><img src="/images/ML/ML_lr_num_data.png" width = "700"></center><br>
ìœ„ì˜ ê·¸ë˜í”„ëŠ” true errorì™€ training errorì˜ **numbers of data** ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚¸ ê²ƒì´ë‹¤. <span style="background-color:#DB9239">xì¶•ì´ model complexityê°€ ì•„ë‹ˆë‹ˆ ì£¼ì˜í•˜ì.</span><br>
<br>
<br>

# â—¼ï¸ Training/test split

ì•ì—ì„œ data set = training set + test set ë¼ê³  í–ˆë‹¤. ê·¸ëŸ¬ë©´ training set ê³¼ test setìœ¼ë¡œ ë‚˜ëˆ ì•¼ í•˜ëŠ”ë° ì–´ë–¤ ë¹„ìœ¨ë¡œ ë‚˜ëˆ ì•¼ í•˜ëŠ”ì§€ê°€ ë¬¸ì œì´ë‹¤.<br>
<center><img src="/images/ML/ML_lr_split.png" width = "700"></center><br>

ìœ„ì˜ ê²½ìš°ì—ëŠ” training set >> test setì¼ ë•Œì¸ë° ì´ëŸ¬ë©´ $\hat{w}$ê°€ ë„ˆë¬´ poorly estimatedë˜ì–´ë²„ë¦°ë‹¤. ì•„ë˜ì˜ ê²½ìš°ëŠ” test errrorê°€ generalization (true) errorë¥¼ ì œëŒ€ë¡œ ëŒ€ë³€í•˜ì§€ ëª»í•œë‹¤.<br>
**($test error \ne appoximation\,of\, generalization$)**<br><br>
ê·¸ë˜ì„œ ì§„ì§œ **"even"**í•˜ê²Œ splitì„ ì˜ í•˜ëŠ”ê²Œ ì¤‘ìš”í•˜ë‹¤.


# â—¼ï¸ 3 sources if error + the bias-variance tradeoff

1. **Noise(Irreducible, ì¤„ì¼ ìˆ˜ ì—†ìŒ)**
2. **Bias(Reducible, ì¤„ì¼ ìˆ˜ ìˆìŒ)**
3. **Variance(Reducible, ì¤„ì¼ ìˆ˜ ìˆìŒ)**
<br>

ìš°ì„  true functionì¸ $f_{w(true)}$ ë¼ëŠ”ê²Œ ìˆë‹¤ê³  ê°€ì •í•˜ì. ê·¸ê²ƒì€ ë¹¨ê°„ìƒ‰ ê·¸ë˜í”„ë¡œ ë‚˜íƒ€ë‚¼ ê²ƒì´ë‹¤.<br>

## Noise
<center><img src="/images/ML/ML_lr_noise.png" width = "700"></center><br>
noiseê°™ì€ ê²½ìš°ëŠ” $\epsilon_{i}$ê°€ ë˜ëŠ”ë° ì´ê²ƒì€ ìš°ë¦¬ê°€ ì¤„ì¼ ìˆ˜ê°€ ì—†ëŠ” ìƒìˆ˜ ê°’ì´ë‹¤. ì–¸ì œë‚˜ í•­ìƒ ì¼ì •í•˜ê²Œ ì¡´ì¬í•˜ê²Œ ëœë‹¤.

## Bias
<center><img src="/images/ML/ML_lr_bias.png" width = "700"></center><br>

Biasê°€ ë§¤ìš° í´ë•Œë¥¼ ìƒê°í•  ê²ƒì´ë‹¤. ê·¸ëŸ¬ë©´ ë§¤ìš° ë‹¨ìˆœí•˜ê²Œ ìƒê¸´ constant functionì´ë¼ê³  ìƒê°í•  ê²ƒì´ë‹¤. ê·¸ëŸ¬ë©´ ìœ„ì™€ ê°™ì´ ì„œë¡œ ë‹¤ë¥¸ training setë“¤ì— ì˜í•´ $f_{\hat{w}}$ê°€ ì—¬ëŸ¬ê°œ ë‚˜ì˜¬ ê²ƒì´ë‹¤. ê·¸ ì¤‘ ì œì¼ expectaioní•œ ê²ƒì„ $f_{\bar{w}}$ë¡œ ë‘˜ ê²ƒì´ë‹¤.<br>
> **<center> $Bias(x) = f_{\bar{w}} - f_{w(true)}$ </center>**

ì´ê²ƒì´ ê·¸ë ‡ê²Œ í•´ì„œ ì–»ì–´ë‚¸ Bias(x)ì˜ ì •ì˜ì´ë‹¤.
<center><img src="/images/ML/ML_lr_exp_bias.png" width = "700"></center><br>
ì—¬ê¸°ì„œ ë³´ë©´ ì•Œìˆ˜ ìˆëŠ” ê²ƒì´ **<span style="background-color:#DB9239">complexityê°€ ë‚®ìœ¼ë©´ biasê°€ ë†’ê³  varianceê°€ ë‚®ë‹¤ëŠ” ê²ƒ</span>**ì´ë‹¤.<br>

## Variance

<center><img src="/images/ML/ML_lr_variance.png" width = "700"></center><br>
ì´ë²ˆì—” varianceê°€ ë§¤ìš° í´ë•Œë¥¼ í™•ì¸í•  ê²ƒì´ë‹¤. ê·¸ë˜ì„œ high-order polynomialë¡œ ìƒê°í•  ê²ƒì´ë‹¤. <br>

<center><img src="/images/ML/ML_lr_high_var.png" width = "700"></center><br>
ê·¸ë˜í”„ê°€ ê±°ì˜ ì¼ì¹˜í•˜ë¯€ë¡œ **<span style="background-color:#DB9239">complexityê°€ ë†’ìœ¼ë©´ biasê°€ ë‚®ê³  varianceê°€ ë†’ë‹¤ëŠ” ê²ƒ</span>**ì´ë‹¤.<br>

## Bias-variance tradeoff

ê·¸ë˜ì„œ ì—¬ê¸°ì„œ ì•Œ ìˆ˜ ìˆëŠ” ê²ƒì€ biasì™€ varianceëŠ” ì„œë¡œ ë°˜ë¹„ë¡€ ê´€ê³„ì— ìˆìŒì„ ì•Œìˆ˜ ìˆë‹¤. ê·¸ë˜ì„œ ê·¸ë˜í”„ë¡œ ê·¸ë ¤ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.<br>
<center><img src="/images/ML/ML_lr_tradeoff.png" width = "700"></center>

<details>
<summary>ğŸ•·ï¸ğŸ•¸ï¸</summary>

<center><img src="/images/ML/great_power.gif" width = "700"></center><br>
</details>
<br>
ì´ê²ƒì€ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤. ê·¸ëŸ°ë° ëŒ€ì²´ ì–´ë””ì„œ ê°‘ìê¸° $Bias^2$ì´ ë‚˜ì˜¨ ê²ƒì¼ê¹Œ?<br>

# â—¼ï¸ Why 3 sources of error? A formal derivation

> **<span style="background-color:#DB9239">Expected prediction error</span> = $E_{<span style="background-color:#0081EB">train</span>}$ [<span style="background-color:#00EB00">generalization error of</span> <span style="background-color:#0081EB">$\hat{w}(train)$</span>]**

<span style="background-color:#DB9239">averaging</span> <span style="background-color:#0081EB">over all training sets and parameters fit on a specific training set</span>

## Deriving expected prediction error

* **Expected predicion error**<br>
    = $E_{train}$[generalization error of $\hat{w}(train)$]<br>
    = $E_{train}[E_{x, y}[L(y, f_{\hat{w}}(x))]]$

    í•œë²ˆì— ì´ê²ƒì„ ê³„ì‚°í•˜ëŠ” ê²ƒì€ ì–´ë ¤ìš°ë‹ˆ ì°¨ê·¼ì°¨ê·¼ í•´ ë³¼ê²ƒì´ë‹¤.
    1. $x_{t}$ì¼ë•Œ ë¶€í„° í•´ë³´ê³ 
    2. $L(y, f_{\hat{w}}(x)) = (y -f_{\hat{w}}(x))^2$ ë¥¼ ê°€ì •í•  ê²ƒì´ë‹¤.(ì´ê²Œ L2-normì´ë‹¤.)

### â€¢ Expected predicion error at $x_{t}$
<center>

= $E_{train,yt}[(y_t -f_{\hat{w}}(x_t))^2]$<br>
<br>

= $E_{train,yt}[(y_t -f_{\hat{w}(train)}(x_t))^2]$<br>
<br>

= $E_{train,yt}[[(y_t-f_{w(true)}(x_t))+(f_{w(true)}(x_t)-f_{\hat{w}(train)}(x_t))]^2]$<br>
<br>
</center>
<span style="color:#DB9239">
\begin{equation}
	= E_{train,yt}[y_t-f_{w(true)}(x_t)]^2
\end{equation}
</span>
<span style="color:#00EB00">
\begin{equation}
	+ 2E_{train,yt}[(y_t-f_{w(true)}(x_t))(f_{w(true)}(x_t)-f_{\hat{w}(train)}(x_t))]
\end{equation}
</span>
<span style="color:#0081EB">
\begin{equation}
	+ E_{train,yt}[f_{w(true)}(x_t)-f_{\hat{w}(train)}(x_t)]^2
\end{equation}
</span>

- - -

ì¼ë‹¨ í•˜ë‚˜í•˜ë‚˜ì”© ë³´ì. <span style="color:#DB9239">(1)</span>ë²ˆ ë¶€í„° ë³¼ê²ƒì´ë‹¤.<br>

<center>
<span style="color:#DB9239">

$E_{train,yt}[y_t-f_{w(true)}(x_t)]^2$<br>
</span>
<br>

= $E_{yt}[(y_t -f_{\hat{w}}(x_t))^2]$ ($\because f_{w(true)}$ì™€ $y_t$ê°€ training dataì— ê´€í•œ í•¨ìˆ˜ê°€ ì•„ë‹ˆê¸° ë•Œë¬¸ì—)<br>
<br>

= $E_{yt}(\epsilon^2)$ ($\because y = f + \epsilon$, $epsilon$: noise )<br>
<br>
</center>

<span style="color:#DB9239">
\begin{equation}
    = E_{yt}(\epsilon^2)
\end{equation}
</span>
<br>
<br>

ê·¸ ë‹¤ìŒì€ <span style="color:#00EB00">(2)</span>ë²ˆ ì´ë‹¤.<br>
<center>
<span style="color:#00EB00">

$2E_{train,yt}[(y_t-f_{w(true)}(x_t))(f_{w(true)}(x_t)-f_{\hat{w}(train)}(x_t))]$<br>
</span>
<br>

= $2E_{train,yt}[\epsilon(f_{w(true)}(x_t)-f_{\hat{w}(train)}(x_t))]$ ($\because y_t-f_{w(true)}(x_t) = \epsilon$)<br>
<br>

= $2E_{train,yt}[\epsilon] \times E_{train,yt}[f_{w(true)}(x_t)-f_{\hat{w}(train)}(x_t)]$ ($\because$ independent)<br>
<br>
= $2E_{yt}[\epsilon] \times E_{train,yt}[f_{w(true)}(x_t)-f_{\hat{w}(train)}(x_t)]$<br>
<br>
= 0 ($\because E_{yt}[\epsilon] = 0$ì´ë¼ê³  ê°€ì •í–ˆì—ˆìŒ)<br>
<br>
</center>

<span style="color: #00EB00">
\begin{equation}
    = 0
\end{equation}
</span>
<br>
<br>

ë§ˆì§€ë§‰ìœ¼ë¡œ <span style="color:#0081EB">(3)</span>ë²ˆ ì´ë‹¤.<br>
<center>
<span style="color:#0081EB">

$E_{train,yt}[f_{w(true)}(x_t)-f_{\hat{w}(train)}(x_t)]^2$<br>
</span>
<br>

= $E_{train}[f_{w(true)}(x_t)-f_{\hat{w}(train)}(x_t)]^2$ ($\because f_{w(true)}$ê°€ $y_t$ì— ê´€í•œ í•¨ìˆ˜ê°€ ì•„ë‹ˆê¸° ë•Œë¬¸ì—)<br>
<br>

</center>

<span style="color: #0081EB">
\begin{equation}
    = E_{train}[f_{w(true)}(x_t)-f_{\hat{w}(train)}(x_t)]^2
\end{equation}
</span>
<br>
<br>

ê·¸ëŸ¬ë©´ ê²°ë¡ ì ìœ¼ë¡œ <span style="color:#DB9239">(1)</span> + <span style="color:#00EB00">(2)</span> + <span style="color:#0081EB">(3)</span>ì€<br><br>

<center>

$E_{yt}(\epsilon^2) + E_{train}[f_{w(true)}(x_t)-f_{\hat{w}(train)}(x_t)]^2$<br>
<br>

= $[E_{yt}(\epsilon^2) - E_{yt}(\epsilon)^2]+ E_{train}[f_{w(true)}(x_t)-f_{\hat{w}(train)}(x_t)]^2$ ($\because E_{yt}(\epsilon) = 0$)<br>
<br>

= $var(\epsilon)+ E_{train}[f_{w(true)}(x_t)-f_{\hat{w}(train)}(x_t)]^2$<br>
<br>

= $\sigma^2 + MSE[f_{w(true)}(x_t)]$ ($\because var(\epsilon) = \sigma^2$)<br>
<br>

</center>

ê°€ ëœë‹¤. ì´ì œ ì—¬ê¸°ì„œ $MSE[f_{w(true)}(x_t)]$ë¥¼ ëœ¯ì–´ë³¼êº¼ë‹¤.<br>

- - -

<center>

$MSE[f_{w(true)}(x_t)]$<br>
<br>

= $E_{train}[f_{w(true)}(x_t)-f_{\hat{w}(train)}(x_t)]^2$<br>
<br>

= $E_{train}[[f_{w(true)}(x_t)-f_{\bar{w}}(x_t)] + [f_{\bar{w}}(x_t)-f_{\hat{w}(train)}(x_t)]]^2$ ($f_{\bar{w}}(x_t) = E_{train}[f_{w(train)}(x_t)]$)<br>
<br>
</center>

<span style="color:#DB9239">
\begin{equation}
	= E_{train}[f_{w(true)}(x_t)-f_{\bar{w}}(x_t)]^2
\end{equation}
</span>
<span style="color:#00EB00">
\begin{equation}
	+ 2E_{train}[(f_{w(true)}(x_t)-f_{\bar{w}}(x_t))(f_{\bar{w}}(x_t)-f_{\hat{w}(train)}(x_t))]
\end{equation}
</span>
<span style="color:#0081EB">
\begin{equation}
	+ E_{train}[f_{\bar{w}}(x_t)-f_{\hat{w}(train)}(x_t)]^2
\end{equation}
</span>

- - -

ì´ê²ƒë„ ìˆœì„œëŒ€ë¡œ í•´ë³´ì. <span style="color:#DB9239">(7)</span>ë²ˆ ë¶€í„° ë³¼ê²ƒì´ë‹¤.<br>

<center>
<span style="color:#DB9239">

$E_{train}[f_{w(true)}(x_t)-f_{\bar{w}}(x_t)]^2$<br>
</span>
<br>

= $[f_{w(true)}(x_t)-f_{\bar{w}}(x_t)]^2$ ($\because [f_{w(true)}(x_t)-f_{\bar{w}}(x_t)]$ is constant)<br>
<br>
</center>

<span style="color:#DB9239">
\begin{equation}
    = [f_{w(true)}(x_t)-f_{\bar{w}}(x_t)]^2
\end{equation}
</span>
<br>
<br>

ê·¸ ë‹¤ìŒì€ <span style="color:#00EB00">(8)</span>ë²ˆ ì´ë‹¤.<br>
<center>
<span style="color:#00EB00">

$2E_{train}[(f_{w(true)}(x_t)-f_{\bar{w}}(x_t))(f_{\bar{w}}(x_t)-f_{\hat{w}(train)}(x_t))]$<br>
</span>
<br>

= $2[f_{w(true)}(x_t)-f_{\bar{w}}(x_t)]E_{train}[f_{\bar{w}}(x_t)-f_{\hat{w}(train)}(x_t)]$<br>
<br>

= $2[f_{w(true)}(x_t)-f_{\bar{w}}(x_t)][E_{train}[f_{\bar{w}}(x_t)]-E_{train}[f_{\hat{w}(train)}(x_t)]]$<br>
<br>
= $2[f_{w(true)}(x_t)-f_{\bar{w}}(x_t)][f_{\bar{w}}(x_t)-f_{\bar{w}}(x_t)]$ ($\because f_{\hat{w}(train)}(x_t) = f_{\bar{w}}(x_t)$)<br>
<br>
= 0 <br>
<br>
</center>

<span style="color: #00EB00">
\begin{equation}
    = 0
\end{equation}
</span>
<br>
<br>

ë§ˆì§€ë§‰ìœ¼ë¡œ <span style="color:#0081EB">(9)</span>ë²ˆ ì´ë‹¤.<br>
<center>
<span style="color:#0081EB">

$E_{train}[f_{\bar{w}}(x_t)-f_{\hat{w}(train)}(x_t)]^2$<br>
</span>
<br>

= $E_{train}[f_{\hat{w}(train)}(x_t) - f_{\bar{w}}(x_t)]^2$<br>
<br>

</center>

<span style="color: #0081EB">
\begin{equation}
    = E_{train}[f_{\hat{w}(train)}(x_t) - f_{\bar{w}}(x_t)]^2
\end{equation}
</span>
<br>
<br>

ê·¸ëŸ¬ë©´ ê²°ë¡ ì ìœ¼ë¡œ <span style="color:#DB9239">(7)</span> + <span style="color:#00EB00">(8)</span> + <span style="color:#0081EB">(9)</span>ì€<br><br>
<center>

$[f_{w(true)}(x_t)-f_{\bar{w}}(x_t)]^2 + E_{train}[f_{\hat{w}(train)}(x_t) - f_{\bar{w}}(x_t)]^2$<br>
<br>

= $[Bias(f_{\hat{w}}(x_t))]^2 + var(f_{\hat{w}(train)}(x_t))$ ($\because f_{\bar{w}}(x_t) = E[f_{\hat{w}(train)}(x_t)]$)<br>
<br>

($Var[X] = E[(X-\mu)^2] = E[X^2] - E[x]^2 = E[X^2] - \mu^2$)
</center><br>
ê°€ ëœë‹¤. ê·¸ëŸ¼ ìµœì¢…ì ìœ¼ë¡œ,

> **<center>Expected prediction error at $x_t$ <br><br>
= $\sigma^2 + MSE[f_{w(true)}(x_t)]$<br><br>
= $\sigma^2 + [Bias(f_{\hat{w}}(x_t))]^2 + var(f_{\hat{w}(train)}(x_t))$</center>**
<br>

ì—¬ê¸°ì—ì„œ ì™œ 3ê°€ì§€ ìš”ì†Œê°€ ìˆê³  ë˜ $bias^2$ ì´ì—ˆëŠ”ì§€ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤.