---
layout: single
title: "ML - Lecture 12: Ensemble learning"
categories: Machine_learning
tags: ML
toc: true
author_profile: false
---
우리가 앞에서 구한 classifier들은 꽤나 좋은 성능을 보여줬다. Variance가 낮고 빠르게 learning을 하는 장점이 있었다. 하지만 bias가 높게나온다는 단점이 있다. 왜냐하면 우리가 true error를 줄이기 위해 weak(simple) classifier를 선택했기 때문이다.
<center><img src="/images/ML/el_weak.png" width = "700"></center><br>

그럼 이러한 weak laerner들을 strong learner로 만들 방법이 없을까? 이걸 바로 ensemble learning: bagging & boosting을 통해 해결할 수 있다.

# ◼︎ Ensemble classifier

Ensemble model은 여러 simple classifier를 coefficient의 가중치에 따라 선형 결헙된 model이다. 우선 simple classifier는 <b>$\hat{y} = f(x) = +1$ or $-1$</b>이다. 그래서 나온 essemble model은 다음과 같다.
> <b><center>F($x_{i}$) = sign($w_1 f_1(x_i) + w_2 f_2(x_i) + ... + w_n f_n(x_i)$)</center></b>

<center><img src="/images/ML/el_ex.png" width = "700"></center><br>
이런식으로 여러개의 simple classifier가 결합된 형태가 나온다. 그래서 최종 공식은 다음과 같다.

> <b><center>$\hat{y} = \operatorname{sign}(\sum_{t=1}^{T}\hat{w_t}f_t(x))$</center></b>

# ◼︎ Bagging

Variance를 줄이는 model을 만들면 생기는 문제점이 있다.
1. Training set이 하나
2. 이렇게 training set이 하나밖에 없는데 어떻게 multiple model을 만들 수 있는 것일까?

그래서 나온것이 Bagging이다. BAGGing은 <u>B</u>ootstrap <u>AGG</u>regation으로 하나만 있던 training set을 통해 data set을 늘리는 방법이라고 생각하면 된다.<br>

## Bagging algorithm

주어진 N개의 training example을 가지고 있는 data set으로 random하게 순서를 바꾼 N개의 training example을 가지고 있는 D'을 만드는 것이다. 

## Random forests

Ensemble method는 여러 decision tree classifier로 이루어 져있다고 하였다. Random forest는 이 여러 tree들을 임의로 뽑아 tree를 새구성 하는 것이라고 생각하면 된다. 그래서 이 임의의 요소로 2가지를 나눈다.
1. **Bagging**: training data의 sample을 random하게 뽑는 것
2. **Random vector**: 모든 feature를 고려하는 것이 아닌 random하게 뽑은 일부의 feature로 split하는 것

<center><img src="/images/ML/el_rf.png" width = "700"></center><br>
Bagging은 variance를 줄여주긴 한다. 하지만 bias를 줄이는 데는 별로 도움이 되지 않는다. 그래서 우리는 boosting을 통해 bias를 줄여주는 것이다.

# ◼︎ Boosting

<center><img src="/images/ML/el_boosting.png" width = "700"></center><br>

이렇게 simple classifier를 evaluating과정을 통해 어디가 틀렸는지를 알게 된다. 이 결과를 통해 **가중치**를 설정해 boosting을 하게 되는 것이다. 이 가중치는 weight의 <b>$w_i$</b> 또는 <b>$\alpha_i$</b>로 설정한다. 그래서 맨 처음은 모든 가중치가 다 같은 1로 설정 한 다음 evaluating을 통해 <span style="color:#3b78f1">정답(O)</span>인 것은 중요도가 <span style="color:#3b78f1">낮아서(↓)</span> 가중치를 <span style="color:#3b78f1">줄이고(↓)</span>, <span style="color:#dd4f4f">틀린(X)</span> 것은 중요도를 <span style="color:#dd4f4f">높이기(↑)</span> 위해 가중치를 <span style="color:#dd4f4f">늘려주면(↑) </span>된다.

# ◼︎ AdaBoost

1. 모든 점이 same weight를 가지게 함: $\alpha_1^i = 1/N$
2. t = 1, ..., T일때의 weight를 계산: $\alpha_i^t$, $\hat{w_t}$, $\alpha_i^{t+1}$
3. $\hat{y} = \operatorname{sign}(\sum_{t=1}^T \hat{w_t}f_t(x))$

## Computing coefficient $\hat{w_t}$

<center><img src="/images/ML/el_w.png" width = "700"></center><br>
이렇게 $f_t(x)$를 평가하여 $\hat{w_t}$를 늘릴리 줄일지 결정하여 새로운 $f_t(x)$를 만드는 것이다. 이렇게 구한 가중치를 이용하여 classification error를 정의 할 수 있다. $f_t(x)$가 좋은 feature이면 가중치를 늘려줘야 할 것이고 좋지 않으면 <span style="color:#3b78f1">낮춰줘야 한다.

> <center> $\operatorname{Weighted classification error}(\epsilon_t) = \frac{\operatorname{total weight of mistakes}}{\operatorname{total weight}} = \frac{\sum_{i=1}^{N} \alpha_i \mathbf{1}(\hat{y_i} \neq y_i) }{\sum_{i=1}^{N} \alpha_i}$ </center>

이렇게 error를 정의 했는데 그러면 $\hat{w_t}$은 어떻게 구할 수 있는 것일까? $\hat{w_t}$은 다음과 같다.
> <center>$\hat{w_t} = \frac{1}{2}ln(\frac{1-\epsilon_t}{\epsilon_t})$</center>

## Recompute weights $\alpha_{t+1}^i$

<center><img src="/images/ML/el_alpha.png" width = "700"></center><br>
그 feature로 구한 $x_i$가 정답이면 <span style="color:#3b78f1">중요도가 낮기 </span>때문에 $\alpha_t^{i}$를 <span style="color:#3b78f1">줄여주고</span> 틀리면 <span style="color:#dd4f4f">중요도가 높아</span> $\alpha_t^{i}$를 <span style="color:#dd4f4f">높여준다</span>.<br>

그러면 $\alpha_{t+1}^{i}$는 다음과 같이 쓸 수 있을 것이다.
> <center>
    $\alpha_{t+1}^{i} = \begin{cases}
    \alpha_{t}^{i}e^{- \hat{w_t}}, \; \mathrm{if} \; f_t(x_i) = y_i \newline
    \alpha_{t}^{i}e^{\hat{w_t}}, \; \mathrm{if} \; f_t(x_i) \neq y_i 
    \end{cases}$ <br><br>
    <span style="color:#DB9239">$ = \alpha_{t}^{i}e^{- \hat{w_t} y_i f_t(x_i)}$</span>
    </center>

 <center><img src="/images/ML/el_adaboost.png"></center><br>

 최종적으로는 이런 식들을 얻을 수 있다.


# ◼︎ Boosting convergence & overfitting


## Overfitting
 <center><img src="/images/ML/el_ab_over.png" width = "700"></center><br>

