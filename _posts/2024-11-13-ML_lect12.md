---
layout: single
title: "ML - Lecture 12: Ensemble learning"
categories: Machine_learning
tags: ML
toc: true
author_profile: false
---
우리가 앞에서 구한 classifier들은 꽤나 좋은 성능을 보여줬다. Variance가 낮고 빠르게 learning을 하는 장점이 있었다. 하지만 bias가 높게나온다는 단점이 있다. 왜냐하면 우리가 true error를 줄이기 위해 weak(simple) classifier를 선택했기 때문이다.
<center><img src="/images/ML/el_weak.png" width = "700"></center><br>

그럼 이러한 weak laerner들을 strong learner로 만들 방법이 없을까? 이걸 바로 ensemble learning: bagging & boosting을 통해 해결할 수 있다.

# ◼︎ Ensemble classifier

Ensemble model은 여러 simple classifier를 coefficient의 가중치에 따라 선형 결헙된 model이다. 우선 simple classifier는 <b>$\hat{y} = f(x) = +1$ or $-1$</b>이다. 그래서 나온 essemble model은 다음과 같다.
> <b><center>F($x_{i}$) = sign($w_1 f_1(x_i) + w_2 f_2(x_i) + ... + w_n f_n(x_i)$)</center></b>

<center><img src="/images/ML/el_ex.png" width = "700"></center><br>
이런식으로 여러개의 simple classifier가 결합된 형태가 나온다. 그래서 최종 공식은 다음과 같다.

> <b><center>$\hat{y} = \operatorname{sign}(\sum_{t=1}^{T}\hat{w_t}f_t(x))$</center></b>

# ◼︎ Bagging

Variance를 줄이는 model을 만들면 생기는 문제점이 있다.
1. Training set이 하나
2. 이렇게 training set이 하나밖에 없는데 어떻게 multiple model을 만들 수 있는 것일까?

